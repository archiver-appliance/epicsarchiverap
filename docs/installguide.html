<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link href="js/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
    <link type="text/css" href="docs.css" rel="Stylesheet" />
    <title>EPICS Archiver Appliance - Install Guide</title>
  </head>
  <body>
    <!-- Header starts here -->
    <nav class="navbar navbar-inverse">
      <div class="container-fluid">
        <div class="navbar-header">
          <a class="navbar-brand" href="index.html"
            >The EPICS Archiver Appliance</a
          >
        </div>
        <div>
          <ul id="mainNavBar" class="nav navbar-nav">
            <li>
              <a href="https://github.com/slacmshankar/epicsarchiverap/wiki"
                >News</a
              >
            </li>
            <li><a href="details.html">Details</a></li>
            <li>
              <a
                href="https://github.com/slacmshankar/epicsarchiverap/releases/"
                >Download</a
              >
            </li>
            <li><a href="quickstart.html">Quickstart</a></li>
            <li><a href="installguide.html">Installation</a></li>
            <li><a href="userguide.html">User Guide</a></li>
            <li><a href="customization.html">Customize</a></li>
            <li><a href="developersguide.html">Developers</a></li>
            <li><a href="admin.html">Admin</a></li>
            <li><a href="api/index.html">Javadoc</a></li>
            <li>
              <a href="https://github.com/slacmshankar/epicsarchiverap/issues"
                >Issues</a
              >
            </li>
            <li><a href="faq.html">FAQ</a></li>
            <li><a href="license.html">License</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li>
              <img
                style="margin-top: -45px"
                src="images/Icon_Mathematical_Plot.png"
              />
            </li>
          </ul>
        </div>
      </div>
    </nav>
    <!-- Header ends here -->

    <div class="container">
      <div id="intro">
        <p>
          If you want to simply test the system and quickly get going, please
          see the <a href="quickstart.html">Quickstart</a> section.
        </p>

        <section>
          <h3>Customize preexisting VM's</h3>
          A simple way to get an installation going is to clone and customize
          Martin's repos for your installation. These consist of three repos
          that are needed to set up the Archiver Appliance environment.
          <ol>
            <li>
              <a href="https://github.com/mark0n/vagrant_archiver_appliance"
                >vagrant_archiver_appliance</a
              >
            </li>
            <li>
              <a href="https://forge.puppet.com/mark0n/epics_archiverappliance"
                >puppet_module_archiver_appliance</a
              >
            </li>
            <li>
              <a href="https://forge.puppet.com/mark0n/epics_softioc"
                >puppet_module_epics_softioc</a
              >
            </li>
          </ol>

          Simply follow the rules in the README of the first repo and the other
          two repos will be pulled in automatically. The Puppet manifests are
          found in puppet_module_archiver_appliance.
        </section>

        <section id="site_specific_installs">
          <h3>Site specific installs</h3>
          Han maintains a set of scripts for
          <a href="https://github.com/jeonghanlee/epicsarchiverap-env"
            >site specific installs</a
          >. This is an excellent starting off point for folks who wish to build
          their own deployment bundles. This is tested against Debian/CentOS;
          but should be easily extensible for other distributions.
        </section>

        <section id="single_machine_install">
          <h3>Using an install script</h3>
          If you plan to have only one machine in the cluster, you can consider
          using the
          <code>install_scripts/single_machine_install.sh</code> install script
          that comes with the installation bundle. This install script
          accommodates installations with a &quot;standard&quot; set of
          parameters and installs the EPICS archiver appliance on one machine.
          In addition to the
          <a href="details.html#SystemRequirements">System requirements</a>, the
          <code>install_scripts/single_machine_install.sh</code> will ask for
          <ol>
            <li>Location of the Tomcat distribution.</li>
            <li>
              Location of the MySQL client jar - usually a file with a name like
              <code>mysql-connector-java-5.1.21-bin.jar</code>
            </li>
            <li>
              A MySQL connection string that looks like so
              <code
                >--user=archappl --password=archappl --database=archappl</code
              >
              that can be used with the MySQL client like so
              <code>mysql ${MYSQL_CONNECTION_STRING} -e "SHOW DATABASES"</code>.
              This implies that the MySQL schema has already been created using
              something like
              <pre>
<code>
mysql --user=root --password=*****
CREATE DATABASE archappl;
GRANT ALL ON archappl.* TO 'archappl' identified by 'archappl';
</code>
</pre>
            </li>
          </ol>
          The <code>install_scripts/single_machine_install.sh</code> install
          script creates a couple of scripts in the deployment folder that can
          be customized for your site.
          <ol>
            <li>
              <b><code>sampleStartup.sh</code></b> - This is a script in the
              fashion of scripts in <code>/etc/init.d</code> that can be used to
              start and stop the four Tomcat processes of your archiver
              appliance.
            </li>
            <li>
              <b><code>deployRelease.sh</code></b> - This can be used to upgrade
              your installation to a new release of the EPICS archiver
              appliance. The <code>deployRelease.sh</code> also includes some
              post install hooks to deploy your site specific content as
              outlined <a href="site_specific_content_changes.html">here</a>.
            </li>
          </ol>
        </section>

        <section id="manual_install">
          <h3>Details</h3>
          For a finer control over your installation, installation and
          configuration consists of these steps. For the cluster
          <ol>
            <li>Create an appliances.xml</li>
            <li>Optionally, create your policies.py file</li>
          </ol>
          In addition to installing the JDK, EPICS (see
          <a href="details.html#SystemRequirements">System requirements</a>),
          for each appliance
          <ol>
            <li>
              Install and configure Tomcat
              <ol>
                <li>
                  Compile the Apache Commons Daemon that is supplied with
                  Tomcat.
                </li>
              </ol>
            </li>
            <li>
              Install MySQL (or other persistence provider)
              <ol>
                <li>Create the tables</li>
                <li>Create a connection pool in Tomcat</li>
              </ol>
            </li>
            <li>Set up storage</li>
            <li>
              Create individual Tomcats for each of the WAR files using the
              provided python script that copies a single Tomcat installation
              into four individual Tomcats - one for each WAR.
            </li>
            <li>
              Deploy the WAR files into their respective containers - This is
              the deployment step that will be run when you upgrade to a new
              release.
            </li>
            <li>Stop/Start each of the Tomcats</li>
          </ol>
        </section>
      </div>

      <section id="appliances_xml">
        <h3>Create an <code>appliances.xml</code></h3>
        The <code>appliances.xml</code> is a file that lists all the appliances
        in a cluster of archiver appliance. While it is not necessary to point
        to the same physical file, the contents are expected to be identical
        across all appliances in the cluster. The details of the file are
        outlined in the
        <a
          href="api/org/epics/archiverappliance/config/ConfigService.html#ARCHAPPL_APPLIANCES"
          >ConfigService</a
        >
        javadoc. A sample <code>appliances.xml</code> with two appliances looks
        like
        <pre class="bash_output"><code>&lt;appliances&gt;
   &lt;appliance&gt;
     &lt;identity&gt;appliance0&lt;/identity&gt;
     &lt;cluster_inetport&gt;archappl0.slac.stanford.edu:16670&lt;/cluster_inetport&gt;
     &lt;mgmt_url&gt;http://archappl0.slac.stanford.edu:17665/mgmt/bpl&lt;/mgmt_url&gt;
     &lt;engine_url&gt;http://archappl0.slac.stanford.edu:17666/engine/bpl&lt;/engine_url&gt;
     &lt;etl_url&gt;http://archappl0.slac.stanford.edu:17667/etl/bpl&lt;/etl_url&gt;
     &lt;retrieval_url&gt;http://archappl0.slac.stanford.edu:17668/retrieval/bpl&lt;/retrieval_url&gt;
     &lt;data_retrieval_url&gt;http://archproxy.slac.stanford.edu/archiver/retrieval&lt;/data_retrieval_url&gt;
   &lt;/appliance&gt;
   &lt;appliance&gt;
     &lt;identity&gt;appliance1&lt;/identity&gt;
     &lt;cluster_inetport&gt;archappl1.slac.stanford.edu:16670&lt;/cluster_inetport&gt;
     &lt;mgmt_url&gt;http://archappl1.slac.stanford.edu:17665/mgmt/bpl&lt;/mgmt_url&gt;
     &lt;engine_url&gt;http://archappl1.slac.stanford.edu:17666/engine/bpl&lt;/engine_url&gt;
     &lt;etl_url&gt;http://archappl1.slac.stanford.edu:17667/etl/bpl&lt;/etl_url&gt;
     &lt;retrieval_url&gt;http://archappl1.slac.stanford.edu:17668/retrieval/bpl&lt;/retrieval_url&gt;
     &lt;data_retrieval_url&gt;http://archproxy.slac.stanford.edu/archiver/retrieval&lt;/data_retrieval_url&gt;
   &lt;/appliance&gt;
 &lt;/appliances&gt;</code></pre>

        <ul>
          <li>
            The archiver appliance looks at the environment variable
            <code>ARCHAPPL_APPLIANCES</code> for the location of the
            <code>appliances.xml</code> file. Use an export statement like so
            <pre>
<code>
export ARCHAPPL_APPLIANCES=/nfs/epics/archiver/production_appliances.xml
</code>
</pre>
            to set the location of the <code>appliances.xml</code> file.
          </li>
          <li>
            The <code>appliances.xml</code> has one
            <code>&lt;appliance&gt;</code> section per appliance. Please only
            define those appliances that are currently in production. Certain
            BPL, most importantly, the <code>/archivePV </code> BPL, are
            suspended until all the appliances defined in the
            <code>appliances.xml</code> have started up and registered their PVs
            in the cluster. Previously, we would allow any number of appliances
            to be defined in the <code>appliances.xml</code> regardless of
            whether they are in production or not. However, it's becoming more
            and more untenable to support this feature. So, from Mar 2023
            onwards, please only define live appliances in
            <code>appliances.xml</code>.
            <strike
              >You can have more entries than you have appliances; that is, if
              you plan to eventually deploy a cluster of 10 machines but only
              have a budget for 2, you can go ahead and add entries for the
              other machines. The cluster should start up even if one or more
              appliances are missing.</strike
            >
          </li>
          <li>
            The <code>identity</code> for each appliance is unique to each
            appliance. For example, the string <code>appliance0</code> serves to
            uniquely identify the archiver appliance on the machine
            <code>archappl0.slac.stanford.edu</code>.
          </li>
          <li>
            The <code>cluster_inetport</code> is the
            <code>TCPIP address:port</code> combination that is used for
            inter-appliance communication. There is a check made to ensure that
            the hostname portion of the <code>cluster_inetport</code> is either
            <code>localhost</code> or the same as that obtained from a call to
            <code>InetAddress.getLocalHost().getCanonicalHostName()</code> which
            typically returns the fully qualified domain name (FQDN). The intent
            here is to prevent multiple appliances starting up with the same
            appliance identity (a situation that could potentially lead to data
            loss).
            <ol>
              <li>
                For a cluster to function correctly, any member
                <code>A</code> of a cluster should be able to communicate with
                any member <code>B</code> of a cluster using <code>B</code>'s
                <code>cluster_inetport</code> as defined in the
                <code> appliances.xml</code>.
              </li>
              <li>
                Obviously, <code>localhost</code> should be used for the
                <code>cluster_inetport</code> only if you have a cluster with
                only one appliance. Even in this case, it's probably more
                future-proof to use the FQDN.
              </li>
            </ol>
          </li>
          <li>
            For the ports, it is convenient if
            <ul>
              <li>
                The port specified in the <code>cluster_inetport</code> is the
                same on all machines. This is the port on which the appliances
                talk to each other.
              </li>
              <li>
                The <code>mgmt_url</code> has the smallest port number amongst
                all the web apps.
              </li>
              <li>
                The port numbers for the other three web apps increment in the
                order show above.
              </li>
            </ul>
            Again, there is no requirement that this be the case. If you follow
            this convention, you can use the standard deployment scripts with
            minimal modification.
          </li>
          <li>
            There are two URL's for the <code>retrieval</code> webapp.
            <ol>
              <li>
                The <code>retrieval_url</code> is the URL used by the
                <code>mgmt</code> webapp to talk to the
                <code>retrieval</code> webapp.
              </li>
              <li>
                The <code>data_retrieval_url</code> is used by archive data
                retrieval clients to talk to the cluster. In this case, we are
                pointing all clients to a single load-balancer on
                <code>archproxy.slac.stanford.edu</code> on port 80. One can use
                the
                <a
                  href="http://httpd.apache.org/docs/2.4/mod/mod_proxy_balancer.html"
                  >mod_proxy_balancer</a
                >
                of Apache to load-balance among any of the appliances in the
                cluster.
                <figure>
                  <img class="img-responsive" src="images/ApacheasLB.png" />
                  <figcaption>
                    Using Apache HTTP on <code>archiver</code> to load balance
                    data retrieval between <code>appliance0</code> and
                    <code>appliance1</code>.
                  </figcaption>
                </figure>
                <ul>
                  <li>
                    Note there are also other load-balancing solutions available
                    that load-balance the HTTP protocol that may be more
                    appropriate for your installation.
                  </li>
                  <li>
                    Also, note that Apache+Tomcat can also use a binary protocol
                    called <code>AJP</code> for load-balancing between Apache
                    and Tomcat. For this software, we should use simple HTTP;
                    this workflow does not entail the additional complexity of
                    the <code>AJP</code> protocol.
                  </li>
                </ul>
              </li>
            </ol>
          </li>
        </ul>
      </section>

      <section id="policies_py">
        <h3>Create your policies file</h3>
        The EPICS archiver appliance ships with a sample
        <a href="customization.html#Policies"><code>policies.py</code></a> (from
        the <code>tests</code> site) that creates a three stage storage
        environment. These are
        <ol>
          <li>
            <b>STS</b> - A datastore that uses the
            <a
              href="api/edu/stanford/slac/archiverappliance/PlainPB/PlainPBStoragePlugin.html"
              >PlainPBStoragePlugin</a
            >
            to store data in a folder specified by the environment variable
            <code>ARCHAPPL_SHORT_TERM_FOLDER</code> at the granularity of an
            hour.
          </li>
          <li>
            <b>MTS</b> - A datastore that uses the
            <a
              href="api/edu/stanford/slac/archiverappliance/PlainPB/PlainPBStoragePlugin.html"
              >PlainPBStoragePlugin</a
            >
            to store data in a folder specified by the environment variable
            <code>ARCHAPPL_MEDIUM_TERM_FOLDER</code> at the granularity of a
            day.
          </li>
          <li>
            <b>LTS</b> - A datastore that uses the
            <a
              href="api/edu/stanford/slac/archiverappliance/PlainPB/PlainPBStoragePlugin.html"
              >PlainPBStoragePlugin</a
            >
            to store data in a folder specified by the environment variable
            <code>ARCHAPPL_LONG_TERM_FOLDER</code> at the granularity of an
            year.
          </li>
        </ol>
        If you are using the generic build and would like to point to a
        different <code>policies.py</code> file, you can use the
        <code>ARCHAPPL_POLICIES</code> environment variable, like so.
        <pre>
<code>
export ARCHAPPL_POLICIES=/nfs/epics/archiver/production_policies.py
</code>
</pre>

        On the other hand, if you are using a site specific build, you can
        bundle your site-specific <code>policies.py</code> as part of the
        <code>mgmt WAR</code> during the site specific build. Just add your
        <code>policies.py</code> to the source code repository under
        <code>src/sitespecific/YOUR_SITE/classpathfiles</code> and build the war
        by setting the <code>ARCHAPPL_SITEID</code> during the build using
        something like <code>export ARCHAPPL_SITEID=YOUR_SITE</code>. In this
        case, you do not need to specify the
        <code>ARCHAPPL_POLICIES</code> environment variable.
      </section>

      <section>
        <h3>Installing Tomcat and setting up Apache Commons Daemon</h3>
        Installing Tomcat consists of
        <ol>
          <li>
            Untar'ing the Tomcat distribution. It is best to set the environment
            variable <code>TOMCAT_HOME</code> to the location where the Tomcat
            distribution is expanded. Many of the following steps require a
            <code>TOMCAT_HOME</code> to be set.
          </li>
          <li>
            Editing the <code>conf/server.xml</code> file to change the ports to
            better suit your installation.
            <ol>
              <li>
                By default, the connector port for the HTTP connector is set to
                8080. Change this to the port used by the
                <code>mgmt</code> webapp for this appliance, in this example,
                17665.
                <pre
                  class="bash_output"
                ><code>&lt;Connector connectionTimeout="20000" port="<span class="rm_txt">8080</span><span class="add_txt">17665</span>" protocol="HTTP/1.1" redirectPort="8443"/&gt;</code></pre>
              </li>
              <li>Remove/comment out the sections for the AJP connector.</li>
              <li>
                At the end, there should be two ports active in the
                <code>conf/server.xml</code> file, one for the HTTP connector
                and the other for the <code>SHUTDOWN</code> command.
              </li>
            </ol>
          </li>
          <li>
            Setting the appropriate log4j configuration level by
            creating/editing the <code>lib/log4j2.xml</code>. Here's a sample
            that logs exceptions and errors with one exception - log messages
            logged to the <code>config</code> namespace are logged at INFO
            level.
            <pre class="bash_output">
<code>&lt;Configuration&gt;
   &lt;Appenders&gt;
        &lt;Console name="STDOUT" target="SYSTEM_OUT"&gt;
            &lt;PatternLayout pattern="%d %-5p [%t] %C{2} (%F:%L) - %m%n"/&gt;
        &lt;/Console&gt;
    &lt;/Appenders&gt;
    &lt;Loggers&gt;
        &lt;Logger name="org.apache.log4j.xml" level="info"/&gt;
        &lt;Root level="info"&gt;
            &lt;AppenderRef ref="STDOUT"/&gt;
        &lt;/Root&gt;
    &lt;/Loggers&gt;
&lt;/Configuration&gt;</code>
</pre>
          </li>
          <li>
            To use
            <a href="http://commons.apache.org/daemon/">Apache Commons Daemon</a
            >, unzip the
            <code>${TOMCAT_HOME}/bin/commons-daemon-native.tar.gz</code> and
            follow the instructions. Once you have built this, copy the
            <code>jsvc</code> binary to the Tomcat <code>bin</code> folder for
            convenience. Note, it's not required that you use
            <code>Apache Commons Daemon</code> especially, if you are already
            using system monitoring and management tools like
            <a href="http://www.nagios.org/">Nagios</a> or
            <a href="http://www.hyperic.com/">Hyperic</a>.

            <pre
              class="bash_output"
            ><code>[ bin ]$ tar zxf commons-daemon-native.tar.gz
[ bin ]$ cd commons-daemon-1.1.0-native-src
[ commons-daemon-1.1.0-native-src ]$ cd unix/
[ unix ]$ ./configure
*** Current host ***
checking build system type... x86_64-pc-linux-gnu
...
[ unix ]$ make
(cd native; make  all)
...
[ unix ]$ cp jsvc ../../../bin/</code></pre>
          </li>
        </ol>
      </section>

      <section>
        <h3>Installing MySQL</h3>
        The version of MySQL that is available from your distribution is
        acceptable; though this is completely untuned. Please look at the more
        than excellent chapters on MySQL optimization at the MySQL web site to
        tune your MySQL instance. In addition to various parameters, even
        something as simple as setting
        <code>innodb_flush_log_at_trx_commit=0</code> (assuming you are ok with
        this) will go a long way in improving performace (especially when
        importing channel archiver configuration files etc). Each appliance has
        its own installation of MySQL. In each appliance,
        <ul>
          <li>
            Make sure MySQL is set to start on powerup (using
            <code>chkconfig</code>)
          </li>
          <li>
            Create a schema for the archiver appliance called
            <code>archappl</code> and grant a user (in this example, also called
            <code>archappl</code>) permissions for this schema.
            <pre>
<code>
CREATE DATABASE archappl;
GRANT ALL ON archappl.* TO 'archappl'@localhost IDENTIFIED BY '&lt;password&gt;';
</code>
</pre>
          </li>
          <li>
            The archiver appliance ships with DDL, for MySQL, this is a file
            called <code>archappl_mysql.sql</code> that is included as part of
            the <code>mgmt</code> WAR file. Execute this script in you newly
            created schema. Confirm that the tables have been created using a
            <code>SHOW TABLES</code> command. There should be at least these
            tables
            <ol>
              <li>
                <code>PVTypeInfo</code> - This table stores the archiving
                parameters for the PVs
              </li>
              <li>
                <code>PVAliases</code> - This table stores EPICS alias mappings
              </li>
              <li>
                <code>ExternalDataServers</code> - This table stores information
                about external data servers.
              </li>
              <li>
                <code>ArchivePVRequests</code> - This table stores archive
                requests that are still pending.
              </li>
            </ol>
          </li>
          <li>
            Download and install the
            <a href="http://dev.mysql.com/downloads/connector/j/"
              >MySQL Connector/J</a
            >
            jar file into your Tomcat's <code>lib</code> folder. In addition to
            the log4j2.xml file, you should have a
            <code>mysql-connector-java-XXX.jar</code> as show here.
            <pre class="bash_output"><code>[ lib ]$ ls -ltra
...
-rw-r--r-- 1 mshankar cd     505 Nov 13 10:29 log4j2.xml
-rw-r--r-- 1 mshankar cd 1007505 Nov 13 10:29 mysql-connector-java-5.1.47-bin.jar
</code></pre>
          </li>
          <li>
            Add a connection pool in Tomcat named <code>jdbc/archappl</code>.
            You can use the Tomcat management UI or directly add an entry in
            <code>conf/context.xml</code> like so
            <pre class="bash_output"><code>
&lt;Resource   name="jdbc/archappl"
      auth="Container"
      type="javax.sql.DataSource"
      factory="org.apache.tomcat.jdbc.pool.DataSourceFactory"
      username="archappl"
      password="XXXXXXX"
      testWhileIdle="true"
      testOnBorrow="true"
      testOnReturn="false"
      validationQuery="SELECT 1"
      validationInterval="30000"
      timeBetweenEvictionRunsMillis="30000"
      maxActive="10"
      minIdle="2"
      maxWait="10000"
      initialSize="2"
      removeAbandonedTimeout="60"
      removeAbandoned="true"
      logAbandoned="true"
      minEvictableIdleTimeMillis="30000"
      jmxEnabled="true"
      driverClassName="com.mysql.jdbc.Driver"
      url="jdbc:mysql://localhost:3306/archappl"
 /&gt;
</code>
</pre>
            Of course, please do make changes appropriate to your installation.
            The only parameter that is fixed is the name of the pool and this
            needs to be <code>jdbc/archappl</code>. All other parameters are
            left to your discretion.
            <ul>
              <li>
                Note for Debian/Ubuntu users: The Tomcat packages shipped with
                Debian/Ubuntu do not include the Tomcat JDBC Connection Pool.
                Download it from the web and drop the
                <code>tomcat-jdbc.jar</code> file into
                <code>/usr/share/tomcat7/lib</code>.
              </li>
            </ul>
          </li>
        </ul>
      </section>

      <section>
        <h3>Setting up storage</h3>
        This is specific to the needs of your <code>policies.py</code>. However,
        if you are using the default <code>policies.py</code> that ships with
        the box or a variant thereof, you'll need to set up three stages of
        storage. A useful way to do this is to create a folder called
        <code>/arch</code> and then create soft links in this folder to the
        actual physical location. For example,
        <pre class="bash_output"><code>[ arch ]$ ls -ltra
total 32
lrwxrwxrwx    1 archappl archappl      8 Jun 21  2013 sts -> /dev/shm
lrwxrwxrwx    1 archappl archappl      4 Jun 21  2013 mts -> data
lrwxrwxrwx    1 archappl archappl     40 Feb 12  2014 lts -> /nfs/site/archappl/archappl01
drwxr-xr-x  195 archappl archappl    4096 Oct 15 15:05 data</code></pre>

        We then set environment variables in the startup script that point to
        the locations within <code>/arch</code>. For example,

        <pre>
<code>
export ARCHAPPL_SHORT_TERM_FOLDER=/arch/sts/ArchiverStore
export ARCHAPPL_MEDIUM_TERM_FOLDER=/arch/mts/ArchiverStore
export ARCHAPPL_LONG_TERM_FOLDER=/arch/lts/ArchiverStore
</code>
</pre>
      </section>

      <section>
        <h3>Create individual Tomcat containers for each of the web apps</h3>
        The <code>mgmt.war</code> file contains a script
        <code>deployMultipleTomcats.py</code> in the <code>install</code> folder
        that will use the information in the <code>appliances.xml</code> file
        and the identity of this appliance to generate individual Tomcat
        containers from a single Tomcat install (identified by the environment
        variable <code>TOMCAT_HOME</code>). To run this script, set the
        following environment variables
        <ol>
          <li>
            <code>TOMCAT_HOME</code> - This is the Tomcat installation that you
            prepared in the previous steps.
          </li>
          <li>
            <code>ARCHAPPL_APPLIANCES</code> - This points to the
            <code>appliances.xml</code> that you created in the previous steps.
          </li>
          <li>
            <code>ARCHAPPL_MYIDENTITY</code> - This is the identity of the
            current appliance, for example <code>appliance0</code>. If this is
            not set, the system will default to using the machine's hostname as
            determined by making a call to
            <code>InetAddress.getLocalHost().getCanonicalHostName()</code>.
            However, this makes <code>ARCHAPPL_MYIDENTITY</code> a physical
            entity and not a logical entity; so, if you can, use a logical name
            for this entry. Note, this must match the
            <code>identity</code> element of this appliance as it is defined in
            the <code><code>appliances.xml</code></code
            >.
          </li>
        </ol>
        and then run the <code>deployMultipleTomcats.py</code> script passing in
        one argument that identifies the parent folder of the individual Tomcat
        containers.
        <pre
          class="bash_output"
        ><code>[ single_machine_install ]$ export TOMCAT_HOME=/arch/single_machine_install/tomcats/apache-tomcat-9.0.20
[ single_machine_install ]$ export ARCHAPPL_APPLIANCES=/arch/single_machine_install/sample_appliances.xml
[ single_machine_install ]$ export ARCHAPPL_MYIDENTITY=appliance0
[ single_machine_install ]$ ./install_scripts/deployMultipleTomcats.py /arch/single_machine_install/tomcats
Using
	tomcat installation at /arch/single_machine_install/tomcats/apache-tomcat-9.0.20
	to generate deployments for appliance appliance0
	using configuration info from /arch/single_machine_install/sample_appliances.xml
	into folder /arch/single_machine_install/tomcats
The start/stop port is the standard Tomcat start/stop port. Changing it to something else random - 16000
The stop/start ports for the new instance will being at  16001
Generating tomcat folder for  mgmt  in location /arch/single_machine_install/tomcats/mgmt
Commenting connector with protocol  AJP/1.3 . If you do need this connector, you should un-comment this.
Generating tomcat folder for  engine  in location /arch/single_machine_install/tomcats/engine
Commenting connector with protocol  AJP/1.3 . If you do need this connector, you should un-comment this.
Generating tomcat folder for  etl  in location /arch/single_machine_install/tomcats/etl
Commenting connector with protocol  AJP/1.3 . If you do need this connector, you should un-comment this.
Generating tomcat folder for  retrieval  in location /arch/single_machine_install/tomcats/retrieval
Commenting connector with protocol  AJP/1.3 . If you do need this connector, you should un-comment this.
[ single_machine_install ]$ </code></pre>

        This is the last of the steps that are install specific; that is, you'll
        execute these only on installation of a new appliance. The remaining
        steps are those that will be executed on deployment of new release,
        start/stop etc.
      </section>

      <section>
        <h3>Deploy the WAR files onto their respective containers</h3>
        Deploying/upgrading a WAR file in a Tomcat container is very easy. Each
        container has a <code>webapps</code> folder; all we have to do is to
        copy the (newer) WAR into this folder and Tomcat (should) will expand
        the WAR file and deploy the WAR file on startup. The deployment/upgrade
        steps are
        <ol>
          <li>Stop all four Tomcat containers.</li>
          <li>
            Remove the older WAR file and expanded WAR file from the
            <code>webapps</code> folder (if present).
          </li>
          <li>Copy the newer WAR file into the <code>webapps</code> folder.</li>
          <li>
            Optionally expand the WAR file after copying it over to the
            <code>webapps</code> folder
            <ul>
              <li>
                This lets you replace individual files in the expanded WAR file
                (for example, images, policies etc) giving you one more way to
                do site specific deployments.
              </li>
            </ul>
          </li>
          <li>Start all four Tomcat containers.</li>
        </ol>
        If <code>DEPLOY_DIR</code> is the parent folder of the individual Tomcat
        containers and <code>WARSRC_DIR</code> is the location where the WAR
        files are present, then the deploy steps (steps 2 and 3 in the list
        above) look something like
        <pre>
<code>
pushd ${DEPLOY_DIR}/mgmt/webapps &amp;&amp; rm -rf mgmt*; cp ${WARSRC_DIR}/mgmt.war .; mkdir mgmt; cd mgmt; jar xf ../mgmt.war; popd;
pushd ${DEPLOY_DIR}/engine/webapps &amp;&amp; rm -rf engine*; cp ${WARSRC_DIR}/engine.war .; mkdir engine; cd engine; jar xf ../engine.war; popd;
pushd ${DEPLOY_DIR}/etl/webapps &amp;&amp; rm -rf etl*; cp ${WARSRC_DIR}/etl.war .; mkdir etl; cd etl; jar xf ../etl.war; popd;
pushd ${DEPLOY_DIR}/retrieval/webapps &amp;&amp; rm -rf retrieval*; cp ${WARSRC_DIR}/retrieval.war .; mkdir retrieval; cd retrieval; jar xf ../retrieval.war; popd;
</code>
</pre>
      </section>

      <section>
        <h3>Stopping and starting the individual Tomcats</h3>
        Running multiple Tomcats on a single machine using the same install
        requires two enviromnent variables
        <ol>
          <li>
            <code>CATALINA_HOME</code> - This is the install folder for Tomcat
            that is common to all Tomcat instances; in our case this is
            <code>$TOMCAT_HOME</code>
          </li>
          <li>
            <code>CATALINA_BASE</code> - This is the deploy folder for Tomcat
            that is specific to each Tomcat instance; in our case this is
            <ul>
              <li><code>${DEPLOY_DIR}/mgmt</code></li>
              <li><code>${DEPLOY_DIR}/etl</code></li>
              <li><code>${DEPLOY_DIR}/engine</code></li>
              <li><code>${DEPLOY_DIR}/retrieval</code></li>
            </ul>
          </li>
        </ol>
        If you are using Apache Commons Daemon, then two bash functions for
        stopping and starting Tomcat instance look something like
        <pre>
<code>
function startTomcatAtLocation() {
    if [ -z "$1" ]; then echo "startTomcatAtLocation called without any arguments"; exit 1; fi
    export CATALINA_HOME=${TOMCAT_HOME}
    export CATALINA_BASE=$1
    echo "Starting tomcat at location ${CATALINA_BASE}"
    pushd ${CATALINA_BASE}/logs
    ${CATALINA_HOME}/bin/jsvc \
        -server \
        -cp ${CATALINA_HOME}/bin/bootstrap.jar:${CATALINA_HOME}/bin/tomcat-juli.jar \
        ${JAVA_OPTS} \
        -Dcatalina.base=${CATALINA_BASE} \
        -Dcatalina.home=${CATALINA_HOME} \
        -cwd ${CATALINA_BASE}/logs \
        -outfile ${CATALINA_BASE}/logs/catalina.out \
        -errfile ${CATALINA_BASE}/logs/catalina.err \
        -pidfile ${CATALINA_BASE}/pid \
        org.apache.catalina.startup.Bootstrap start
     popd
}

function stopTomcatAtLocation() {
    if [ -z "$1" ]; then echo "stopTomcatAtLocation called without any arguments"; exit 1; fi
    export CATALINA_HOME=${TOMCAT_HOME}
    export CATALINA_BASE=$1
    echo "Stopping tomcat at location ${CATALINA_BASE}"
    pushd ${CATALINA_BASE}/logs
    ${CATALINA_HOME}/bin/jsvc \
        -server \
        -cp ${CATALINA_HOME}/bin/bootstrap.jar:${CATALINA_HOME}/bin/tomcat-juli.jar \
        ${JAVA_OPTS} \
        -Dcatalina.base=${CATALINA_BASE} \
        -Dcatalina.home=${CATALINA_HOME} \
        -cwd ${CATALINA_BASE}/logs \
        -outfile ${CATALINA_BASE}/logs/catalina.out \
        -errfile ${CATALINA_BASE}/logs/catalina.err \
        -pidfile ${CATALINA_BASE}/pid \
        -stop \
        org.apache.catalina.startup.Bootstrap
     popd
}
</code>
</pre>

        and you'd invoke these using something like
        <pre>
<code>
stopTomcatAtLocation ${DEPLOY_DIR}/engine
stopTomcatAtLocation ${DEPLOY_DIR}/retrieval
stopTomcatAtLocation ${DEPLOY_DIR}/etl
stopTomcatAtLocation ${DEPLOY_DIR}/mgmt
</code>
</pre>

        and

        <pre>
<code>
startTomcatAtLocation ${DEPLOY_DIR}/mgmt
startTomcatAtLocation ${DEPLOY_DIR}/engine
startTomcatAtLocation ${DEPLOY_DIR}/etl
startTomcatAtLocation ${DEPLOY_DIR}/retrieval

</code>
</pre>

        Remember to set all the appropriate environment variables from the
        previous steps
        <ol>
          <li><code>JAVA_HOME</code></li>
          <li><code>TOMCAT_HOME</code></li>
          <li><code>ARCHAPPL_APPLIANCES</code></li>
          <li><code>ARCHAPPL_MYIDENTITY</code></li>
          <li><code>ARCHAPPL_SHORT_TERM_FOLDER</code> or equivalent</li>
          <li><code>ARCHAPPL_MEDIUM_TERM_FOLDER</code> or equivalent</li>
          <li><code>ARCHAPPL_LONG_TERM_FOLDER</code> or equivalent</li>
          <li>
            <code>JAVA_OPTS</code> - This is the environment variable typically
            used by Tomcat to pass arguments to the VM. You can pass in
            appropriate arguments like so
            <pre><code>export JAVA_OPTS="-XX:+UseG1GC -Xmx4G -Xms4G -ea"</code></pre>
          </li>
          <li>
            <code>LD_LIBRARY_PATH</code> - If you are using JCA, please make
            sure your LD_LIBRARY_PATH includes the paths to the JCA and EPICS
            base <code>.so</code>'s.
          </li>
        </ol>

        A sample startup script using these elements is available
        <a href="samples/sampleStartup.sh">here</a>. Please modify to suit your
        installation.
      </section>

      <section>
        <h3>Other containers</h3>
        It is possible to deploy the 4 WAR files of the archiver appliance on
        other servlet containers or to use other industry standard provisioning
        software to provision an appliance. The details outlined here are
        guidelines on how to provision an appliance using Tomcat as a servlet
        container. If you generate scripts for industry standard provisioning
        software and are willing to share them, please add them to the
        repositorty and contact the collaboration; we'll be happy to modify
        these documents to accomodate the same.
      </section>
    </div>

    <script src="js/jquery/1.11.3/jquery.min.js"></script>
    <script src="js/bootstrap-3.3.5/js/bootstrap.min.js"></script>
    <script src="js/arch/docs.js"></script>
  </body>
</html>
