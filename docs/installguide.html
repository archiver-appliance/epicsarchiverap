<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="js/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet">
<link type="text/css" href="docs.css" rel="Stylesheet" />	
<title>EPICS Archiver Appliance - Install Guide</title>
</head>
<body>
<!-- Header starts here -->
 <nav class="navbar navbar-inverse">
  <div class="container-fluid">
    <div class="navbar-header">
      <a class="navbar-brand" href="index.html">The EPICS Archiver Appliance</a>
    </div>
    <div>
      <ul id="mainNavBar" class="nav navbar-nav">
		<li><a href="https://github.com/slacmshankar/epicsarchiverap/wiki">News</a></li>
		<li><a href="details.html">Details</a></li>
		<li><a href="https://github.com/slacmshankar/epicsarchiverap/releases/">Download</a></li>
		<li><a href="quickstart.html">Quickstart</a></li>
		<li><a href="installguide.html">Installation</a></li>
		<li><a href="userguide.html">User Guide</a></li>
		<li><a href="customization.html">Customize</a></li>
		<li><a href="developersguide.html">Developers</a></li>
		<li><a href="admin.html">Admin</a></li>
		<li><a href="api/index.html">Javadoc</a></li>
		<li><a href="https://github.com/slacmshankar/epicsarchiverap/issues">Issues</a></li>
		<li><a href="faq.html">FAQ</a></li>
		<li><a href="license.html">License</a></li>
	  </ul>
	  <ul class="nav navbar-nav navbar-right">
   	    <li><img style="margin-top: -45px" src="images/Icon_Mathematical_Plot.png"/></li>
	  </ul>
    </div>
  </div>
</nav>
<!-- Header ends here -->

<div class="container">

<div id="intro">
<p>If you want to simply test the system and quickly get going, please see the <a href="quickstart.html">Quickstart</a> section.</p>

<section>
<h3>Customize preexisting VM's</h3>
A simple way to get an installation going is to clone and customize Martin's repos for your installation. 
These consist of three repos that are needed to set up the Archiver Appliance  environment.
<ol>
<li><a href="https://stash.nscl.msu.edu/projects/DEPLOY/repos/vagrant_archiver_appliance">vagrant_archiver_appliance</a></li>
<li><a href="https://stash.nscl.msu.edu/projects/DEPLOY/repos/puppet_module_archiver_appliance">puppet_module_archiver_appliance</a></li>
<li><a href="https://stash.nscl.msu.edu/projects/DEPLOY/repos/puppet_module_epics_softioc">puppet_module_epics_softioc</a></li>
</ol>

Simply follow the rules in the README of the first repo and the other two repos will be pulled in automatically. The Puppet manifests are found in puppet_module_archiver_appliance.
</section>



<section id="single_machine_install">
<h3>Using an install script</h3>
If you plan to have only one machine in the cluster, you can consider using the <code>install_scripts/single_machine_install.sh</code> install script that comes with the installation bundle.
This install script accommodates installations with a &quot;standard&quot; set of parameters and installs the EPICS archiver appliance on one machine.
In addition to the <a href="details.html#SystemRequirements">System requirements</a>, the <code>install_scripts/single_machine_install.sh</code> will ask for 
<ol>
<li>Location of the Tomcat distribution.</li>
<li>Location of the MySQL client jar - usually a file with a name like <code>mysql-connector-java-5.1.21-bin.jar</code></li>
<li>A MySQL connection string that looks like so <code>--user=archappl --password=archappl --database=archappl</code> that can be used with the MySQL client like so <code>mysql ${MYSQL_CONNECTION_STRING} -e "SHOW DATABASES"</code>.
This implies that the MySQL schema has already been created using something like
<pre>
<code>
mysql --user=root --password=*****
CREATE DATABASE archappl;
GRANT ALL ON archappl.* TO 'archappl' identified by 'archappl';
</code> 
</pre>
</li>
</ol>
The <code>install_scripts/single_machine_install.sh</code> install script creates a couple of scripts in the deployment folder that can be customized for your site.
<ol>
<li><b><code>sampleStartup.sh</code></b> - This is a script in the fashion of scripts in <code>/etc/init.d</code> that can be used to start and stop the four Tomcat processes of your archiver appliance.</li>
<li><b><code>deployRelease.sh</code></b> - This can be used to upgrade your installation to a new release of the EPICS archiver appliance.
The <code>deployRelease.sh</code> also includes some post install hooks to deploy your site specific content as outlined <a href="site_specific_content_changes.html">here</a>.
</li>
</ol>
</section>


<section id="manual_install">
<h3>Details</h3>
For a finer control over your installation, installation and configuration consists of these steps.
For the cluster
<ol>
<li>Create an appliances.xml</li>
<li>Optionally, create your policies.py file</li>
</ol>
In addition to installing the Sun Java 8 JDK, EPICS (see <a href="details.html#SystemRequirements">System requirements</a>), for each appliance
<ol>
<li>Install and configure Tomcat<ol>
<li>Compile the Apache Commons Daemon that is supplied with Tomcat.</li>
</ol></li>
<li>Install MySQL (or other persistence provider)
<ol>
<li>Create the tables</li>
<li>Create a connection pool in Tomcat</li>
</ol></li>
<li>Set up storage</li>
<li>Create individual Tomcats for each of the WAR files using the provided python script that copies a single Tomcat installation into four individual Tomcats - one for each WAR.</li>
<li>Deploy the WAR files into their respective containers - This is the deployment step that will be run when you upgrade to a new release.</li>
<li>Stop/Start each of the Tomcats</li>
</ol>
</div>
</section>

<section id="appliances_xml">
<h3>Create an <code>appliances.xml</code></h3>
The <code>appliances.xml</code> is a file that lists all the appliances in a cluster of archiver appliance.
While it is not necessary to point to the same physical file, the contents are expected to be identical across all appliances in the cluster.
The details of the file are outlined in the <a href="api/org/epics/archiverappliance/config/ConfigService.html#ARCHAPPL_APPLIANCES">ConfigService</a> javadoc.
A sample  <code>appliances.xml</code> with two appliances looks like 
<figure>
<img src="images/appliancesxml.png"/>
<figcaption>The <code>appliances.xml</code> lists all the appliances in the cluster.</figcaption>
</figure> 

<ul>
<li>The archiver appliance looks at the environment variable <code>ARCHAPPL_APPLIANCES</code> for the location of the <code>appliances.xml</code> file. Use an export statement like so
<pre>
<code>
export ARCHAPPL_APPLIANCES=/nfs/epics/archiver/production_appliances.xml
</code>
</pre>
to set the location of the <code>appliances.xml</code> file.
</li>
<li>
The <code>appliances.xml</code> has one <code>&lt;appliance&gt;</code> section per appliance. 
You can have more entries than you have appliances; that is, if you plan to eventually deploy a cluster of 10 machines but only have a budget for 2, you can go ahead and add entries for the other machines. 
The cluster should start up even if one or more appliances are missing.
</li>
<li>
The <code>identity</code> for each appliance is unique to each appliance. 
For example, the string <code>appliance0</code> serves to uniquely identify the archiver appliance on the machine <code>archappl0.slac.stanford.edu</code>.
</li>
<li>The <code>cluster_inetport</code> is the <code>TCPIP address:port</code> combination that is used for inter-appliance communication. 
There is a check made to ensure that the hostname portion of the <code>cluster_inetport</code> is either <code>localhost</code> or the same as that obtained from a call to <code>InetAddress.getLocalHost().getCanonicalHostName()</code> which typically returns the fully qualified domain name (FQDN).
The intent here is to prevent multiple appliances starting up with the same identity.
<ol>
<li>For a cluster to function correctly, any member <code>A</code> of a cluster should be able to communicate with any member <code>B</code> of a cluster using <code>B</code>'s <code>cluster_inetport</code> as defined in the <code> appliances.xml</code>.</li>
<li>Obviously, <code>localhost</code> should be used for the <code>cluster_inetport</code> only if you have a cluster with only one appliance. Even in this case, it's probably more future-proof to use the FQDN.</li>
</ol>
</li>
<li>
For the ports, it is convenient if
<ul>
<li>The port specified in the <code>cluster_inetport</code> is the same on all machines. This is the port on which the appliances talk to each other.</li>
<li>The <code>mgmt_url</code> has the smallest port number amongst all the web apps.</li>
<li>The port numbers for the other three web apps increment in the order show above.</li>
</ul>
Again, there is no requirement that this be the case.
If you follow this convention, you can use the standard deployment scripts with minimal modification.
</li>
<li>
There are two URL's for the <code>retrieval</code> webapp.
<ol>
<li>The <code>retrieval_url</code> is the URL used by the <code>mgmt</code> webapp to talk to the <code>retrieval</code> webapp.</li>
<li>The <code>data_retrieval_url</code> is used by archive data retrieval clients to talk to the cluster.
In this case, we are pointing all clients to a single load-balancer on <code>archiver.slac.stanford.edu</code> on port 80.
One can use the <a href="http://httpd.apache.org/docs/2.4/mod/mod_proxy_balancer.html">mod_proxy_balancer</a> of Apache to load-balance among any of the appliances in the cluster.
<figure>
<img src="images/ApacheasLB.png"/>
<figcaption>Using Apache HTTP on <code>archiver</code> to load balance data retrieval between <code>appliance0</code> and <code>appliance1</code>.</figcaption>
</figure> 
<ul>
<li>Note there are also other load-balancing solutions available that load-balance the HTTP protocol that may be more appropriate for your installation.</li>
<li>Also, note that Apache+Tomcat can also use a binary protocol called <code>AJP</code> for load-balancing between Apache and Tomcat. 
For this software, we should use simple HTTP; this workflow does not entail the additional complexity of the <code>AJP</code> protocol.
</li>
</ul>

</li>
</ol>
</li>
</ul>

</section>

<section id="policies_py">
<h3>Create your policies file</h3>
The EPICS archiver appliance ships with a sample <a href="customization.html#Policies"><code>policies.py</code></a> (from the <code>tests</code> site) that creates a three stage storage environment. These are 
<ol>
<li><b>STS</b> - A datastore that uses the <a href="api/edu/stanford/slac/archiverappliance/PlainPB/PlainPBStoragePlugin.html">PlainPBStoragePlugin</a> to store data in a folder specified by the environment variable <code>ARCHAPPL_SHORT_TERM_FOLDER</code> at the granularity of an hour.
</li>
<li><b>MTS</b> - A datastore that uses the <a href="api/edu/stanford/slac/archiverappliance/PlainPB/PlainPBStoragePlugin.html">PlainPBStoragePlugin</a> to store data in a folder specified by the environment variable <code>ARCHAPPL_MEDIUM_TERM_FOLDER</code> at the granularity of a day.
</li>
<li><b>LTS</b> - A datastore that uses the <a href="api/edu/stanford/slac/archiverappliance/PlainPB/PlainPBStoragePlugin.html">PlainPBStoragePlugin</a> to store data in a folder specified by the environment variable <code>ARCHAPPL_LONG_TERM_FOLDER</code> at the granularity of an year.
</li>
</ol>
If you are using the generic build and would like to point to a different <code>policies.py</code> file, you can use the <code>ARCHAPPL_POLICIES</code> environment variable, like so.
<pre>
<code>
export ARCHAPPL_POLICIES=/nfs/epics/archiver/production_policies.py
</code>
</pre>

On the other hand, if you are using a site specific build, you can bundle your site-specific <code>policies.py</code> as part of the <code>mgmt WAR</code> during the site specific build.
Just add your <code>policies.py</code> to the source code repository under <code>src/sitespecific/YOUR_SITE/classpathfiles</code> and build the war by setting the <code>ARCHAPPL_SITEID</code> during the build using something like <code>export ARCHAPPL_SITEID=YOUR_SITE</code>.
In this case, you do not need to specify the <code>ARCHAPPL_POLICIES</code> environment variable.

</section>

<section>
<h3>Installing Tomcat and setting up Apache Commons Daemon</h3>
Installing Tomcat consists of 
<ol>
<li>Untar'ing the Tomcat distribution. It is best to set the environment variable <code>TOMCAT_HOME</code> to the location where the Tomcat distribution is expanded.
Many of the following steps require a <code>TOMCAT_HOME</code> to be set.
</li>
<li>Editing the <code>conf/server.xml</code> file to change the ports to better suit your installation.
<ol>
<li>By default, the connector port for the HTTP connector is set to 8080. 
Change this to the port used by the <code>mgmt</code> webapp for this appliance, in this example, 17665.
<figure>
<img src="images/tomcathttpconnector.png"/>
<figcaption>Change 8080 to the port used by the <code>mgmt</code> webapp for this appliance.</figcaption>
</figure> 
</li>
<li>
Remove/comment out the sections for the AJP connector. 
</li>
<li>
At the end, there should be two ports active in the <code>conf/server.xml</code> file, one for the HTTP connector and the other for the <code>SHUTDOWN</code> command.
</li>
</ol>
</li>
<li>Setting the appropriate log4j configuration level by creating/editing the <code>lib/log4j.properties</code>.
Here's a sample that logs exceptions and errors with one exception - log messages logged to the <code>config</code> namespace are logged at INFO level. 
<pre>
<code>
# Set root logger level and its only appender to A1.
log4j.rootLogger=ERROR, A1
log4j.logger.config.org.epics.archiverappliance=INFO
log4j.logger.org.apache.http=ERROR


# A1 is set to be a DailyRollingFileAppender
log4j.appender.A1=org.apache.log4j.DailyRollingFileAppender
log4j.appender.A1.File=arch.log
log4j.appender.A1.DatePattern='.'yyyy-MM-dd


# A1 uses PatternLayout.
log4j.appender.A1.layout=org.apache.log4j.PatternLayout
log4j.appender.A1.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n
</code>
</pre>

</li>
<li>To use <a href="http://commons.apache.org/daemon/">Apache Commons Daemon</a>, unzip the <code>bin/commons-daemon-native.tar.gz</code> and follow the instructions.
Once you have built this, copy the <code>jsvc</code> binary to the Tomcat <code>bin</code> folder for convenience.
Note, it's not required that you use <code>Apache Commons Daemon</code> especially, if you are already using system monitoring and management tools like <a href="http://www.nagios.org/">Nagios</a> or <a href="http://www.hyperic.com/">Hyperic</a>.
<figure>
<img src="images/buildingjsvc.png"/>
<figcaption>Building Apache Commons Daemon.</figcaption>
</figure> 
</li>
</ol>

</section>

<section>
<h3>Installing MySQL</h3>
The version of MySQL that is available from your distribution is acceptable; though this is completely untuned. Please look at the more than excellent chapters on MySQL optimization at the MySQL web site to tune your MySQL instance.
In addition to various parameters, even something as simple as setting <code>innodb_flush_log_at_trx_commit=0</code> (assuming you are ok with this) will go a long way in improving performace (especially when importing channel archiver configuration files etc).
Each appliance has its own installation of MySQL. In each appliance,   
<ul>
<li>
Make sure MySQL is set to start on powerup (using <code>chkconfig</code>)
</li>
<li>Create a schema for the archiver appliance called <code>archappl</code> and grant a user (in this example, also called <code>archappl</code>) permissions for this schema.
<pre>
<code>
CREATE DATABASE archappl;
GRANT ALL ON archappl.* TO 'archappl'@localhost IDENTIFIED BY '&lt;password&gt;';
</code>
</pre>
</li>
<li>
The archiver appliance ships with DDL, for MySQL, this is a file called <code>archappl_mysql.sql</code> that is included as part of the <code>mgmt</code> WAR file. 
Execute this script in you newly created schema.
Confirm that the tables have been created using a <code>SHOW TABLES</code> command.
There should be at least these tables
<ol>
<li><code>PVTypeInfo</code> - This table stores the archiving parameters for the PVs</li>
<li><code>PVAliases</code> - This table stores EPICS alias mappings</li>
<li><code>ExternalDataServers</code> - This table stores information about external data servers.</li>
<li><code>ArchivePVRequests</code> - This table stores archive requests that are still pending.</li>
</ol> 
</li>
<li>Download and install the <a href="http://dev.mysql.com/downloads/connector/j/">MySQL Connector/J</a> jar file into your Tomcat's <code>lib</code> folder.
In addition to the log4j.properties file, you should have a <code>mysql-connector-java-XXX.jar</code> as show here.

<figure>
<img src="images/tomcatlib1.png"/>
<figcaption>Tomcat lib after copying over MySQL jar</figcaption>
</figure> 
</li>
<li>
Add a connection pool in Tomcat named <code>jdbc/archappl</code>.  
You can use the Tomcat management UI or directly add an entry in <code>conf/context.xml</code> like so
<pre>
<code>
&lt;Resource   name="jdbc/archappl"
      auth="Container"
      type="javax.sql.DataSource"
      factory="org.apache.tomcat.jdbc.pool.DataSourceFactory"
      username="archappl"
      password="XXXXXXX" 
      testWhileIdle="true"
      testOnBorrow="true"
      testOnReturn="false"
      validationQuery="SELECT 1"
      validationInterval="30000"
      timeBetweenEvictionRunsMillis="30000"
      maxActive="10" 
      minIdle="2" 
      maxWait="10000" 
      initialSize="2"
      removeAbandonedTimeout="60"
      removeAbandoned="true"
      logAbandoned="true"
      minEvictableIdleTimeMillis="30000" 
      jmxEnabled="true"
      driverClassName="com.mysql.jdbc.Driver"
      url="jdbc:mysql://localhost:3306/archappl"
 /&gt;
</code>
</pre>
Of course, please do make changes appropriate to your installation. 
The only parameter that is fixed is the name of the pool and this needs to be <code>jdbc/archappl</code>.
All other parameters are left to your discretion.
<figure>
<img src="images/connpool.png"/>
</figure>
<ul>
<li>Note for Debian/Ubuntu users: The Tomcat packages shipped with Debian/Ubuntu do not include the Tomcat JDBC Connection Pool. Download it from the web and drop the <code>tomcat-jdbc.jar</code> file into <code>/usr/share/tomcat7/lib</code>.</li>
</ul>
</li>
</ul>

</section>


<section>
<h3>Setting up storage</h3>
This is specific to the needs of your <code>policies.py</code>. 
However, if you are using the default <code>policies.py</code> that ships with the box or a variant thereof, you'll need to set up three stages of storage.
A useful way to do this is to create a folder called <code>/arch</code> and then create soft links in this folder to the actual physical location.
For example,
<figure>
<img src="images/slasharch.png"/>
<figcaption><code>/arch</code> showing soft links to actual storage locations.</figcaption>
</figure> 
We then set environment variables in the startup script that point to the locations within <code>/arch</code>.
For example,

<pre>
<code>
export ARCHAPPL_SHORT_TERM_FOLDER=/arch/sts/ArchiverStore
export ARCHAPPL_MEDIUM_TERM_FOLDER=/arch/mts/ArchiverStore
export ARCHAPPL_LONG_TERM_FOLDER=/arch/lts/ArchiverStore
</code>
</pre>
</section>

<section>
<h3>Create individual Tomcat containers for each of the web apps</h3>
The <code>mgmt.war</code> file contains a script <code>deployMultipleTomcats.py</code> in the <code>install</code> folder that will use the information in the <code>appliances.xml</code> file and the identity of this appliance to generate individual Tomcat containers from a single Tomcat install (identified by the environment variable <code>TOMCAT_HOME</code>).
To run this script, set the following environment variables
<ol>
<li><code>TOMCAT_HOME</code> - This is the Tomcat installation that you prepared in the previous steps.</li>
<li><code>ARCHAPPL_APPLIANCES</code> - This points to the <code>appliances.xml</code> that you created in the previous steps.</li>
<li><code>ARCHAPPL_MYIDENTITY</code> - This is the identity of the current appliance, for example <code>appliance0</code>. 
If this is not set, the system will default to using the machine's hostname as determined by making a call to <code>InetAddress.getLocalHost().getCanonicalHostName()</code>.
However, this makes <code>ARCHAPPL_MYIDENTITY</code> a physical entity and not a logical entity; so, if you can, use a logical name for this entry.
Note, this must match the <code>identity</code> element of this appliance as it is defined in the <code><code>appliances.xml</code></code>.
</li>
</ol>
and then run the <code>deployMultipleTomcats.py</code> script passing in one argument that identifies the parent folder of the individual Tomcat containers.
<figure>
<img src="images/deployMultiple.png"/>
<figcaption>Using the <code>deployMultipleTomcats.py</code> script to create individual containers for each WAR file.</figcaption>
</figure> 
This is the last of the steps that are install specific; that is, you'll execute these only on installation of a new appliance.
The remaining steps are those that will be executed on deployment of new release, start/stop etc.
</section>

<section>
<h3>Deploy the WAR files onto their respective containers</h3>
Deploying/upgrading a WAR file in a Tomcat container is very easy. 
Each container has a <code>webapps</code> folder; all we have to do is to copy the (newer) WAR into this folder and Tomcat (should) will expand the WAR file and deploy the WAR file on startup. 
The deployment/upgrade steps are
<ol>
<li>Stop all four Tomcat containers.</li>
<li>Remove the older WAR file and expanded WAR file from the <code>webapps</code> folder (if present).</li>
<li>Copy the newer WAR file into the <code>webapps</code> folder.</li>
<li>Optionally expand the WAR file after copying it over to the <code>webapps</code> folder
<ul>
<li>This lets you replace individual files in the expanded WAR file (for example, images, policies etc) giving you one more way to do site specific deployments.</li>
</ul>
</li>
<li>Start all four Tomcat containers.</li>
</ol>
If <code>DEPLOY_DIR</code> is the parent folder of the individual Tomcat containers and <code>WARSRC_DIR</code> is the location where the WAR files are present, then the deploy steps (steps 2 and 3 in the list above) look something like
<pre>
<code>
pushd ${DEPLOY_DIR}/mgmt/webapps &amp;&amp; rm -rf mgmt*; cp ${WARSRC_DIR}/mgmt.war .; mkdir mgmt; cd mgmt; jar xf ../mgmt.war; popd; 
pushd ${DEPLOY_DIR}/engine/webapps &amp;&amp; rm -rf engine*; cp ${WARSRC_DIR}/engine.war .; mkdir engine; cd engine; jar xf ../engine.war; popd; 
pushd ${DEPLOY_DIR}/etl/webapps &amp;&amp; rm -rf etl*; cp ${WARSRC_DIR}/etl.war .; mkdir etl; cd etl; jar xf ../etl.war; popd; 
pushd ${DEPLOY_DIR}/retrieval/webapps &amp;&amp; rm -rf retrieval*; cp ${WARSRC_DIR}/retrieval.war .; mkdir retrieval; cd retrieval; jar xf ../retrieval.war; popd;
</code>
</pre>
</section>

<section>
<h3>Stopping and starting the individual Tomcats</h3>
Running multiple Tomcats on a single machine using the same install requires two enviromnent variables
<ol>
<li><code>CATALINA_HOME</code> - This is the install folder for Tomcat that is common to all Tomcat instances; in our case this is <code>$TOMCAT_HOME</code></li>
<li><code>CATALINA_BASE</code> - This is the deploy folder for Tomcat that is specific to each Tomcat instance; in our case this is 
<ul><li><code>${DEPLOY_DIR}/mgmt</code></li><li><code>${DEPLOY_DIR}/etl</code></li><li><code>${DEPLOY_DIR}/engine</code></li><li><code>${DEPLOY_DIR}/retrieval</code></li></ul></li>
</ol>
If you are using Apache Commons Daemon, then two bash functions for stopping and starting Tomcat instance look something like
<pre>
<code>
function startTomcatAtLocation() { 
    if [ -z "$1" ]; then echo "startTomcatAtLocation called without any arguments"; exit 1; fi
    export CATALINA_HOME=${TOMCAT_HOME}
    export CATALINA_BASE=$1
    echo "Starting tomcat at location ${CATALINA_BASE}"
    pushd ${CATALINA_BASE}/logs
    ${CATALINA_HOME}/bin/jsvc \
        -server \
        -cp ${CATALINA_HOME}/bin/bootstrap.jar:${CATALINA_HOME}/bin/tomcat-juli.jar \
        ${JAVA_OPTS} \
        -Dcatalina.base=${CATALINA_BASE} \
        -Dcatalina.home=${CATALINA_HOME} \
        -cwd ${CATALINA_BASE}/logs \
        -outfile ${CATALINA_BASE}/logs/catalina.out \
        -errfile ${CATALINA_BASE}/logs/catalina.err \
        -pidfile ${CATALINA_BASE}/pid \
        org.apache.catalina.startup.Bootstrap start
     popd
}

function stopTomcatAtLocation() { 
    if [ -z "$1" ]; then echo "stopTomcatAtLocation called without any arguments"; exit 1; fi
    export CATALINA_HOME=${TOMCAT_HOME}
    export CATALINA_BASE=$1
    echo "Stopping tomcat at location ${CATALINA_BASE}"
    pushd ${CATALINA_BASE}/logs
    ${CATALINA_HOME}/bin/jsvc \
        -server \
        -cp ${CATALINA_HOME}/bin/bootstrap.jar:${CATALINA_HOME}/bin/tomcat-juli.jar \
        ${JAVA_OPTS} \
        -Dcatalina.base=${CATALINA_BASE} \
        -Dcatalina.home=${CATALINA_HOME} \
        -cwd ${CATALINA_BASE}/logs \
        -outfile ${CATALINA_BASE}/logs/catalina.out \
        -errfile ${CATALINA_BASE}/logs/catalina.err \
        -pidfile ${CATALINA_BASE}/pid \
        -stop \
        org.apache.catalina.startup.Bootstrap 
     popd
}
</code>
</pre>

and you'd invoke these using something like
<pre>
<code>
stopTomcatAtLocation ${DEPLOY_DIR}/engine
stopTomcatAtLocation ${DEPLOY_DIR}/retrieval
stopTomcatAtLocation ${DEPLOY_DIR}/etl
stopTomcatAtLocation ${DEPLOY_DIR}/mgmt
</code>
</pre> 

and 

<pre>
<code>
startTomcatAtLocation ${DEPLOY_DIR}/mgmt
startTomcatAtLocation ${DEPLOY_DIR}/engine
startTomcatAtLocation ${DEPLOY_DIR}/etl
startTomcatAtLocation ${DEPLOY_DIR}/retrieval

</code>
</pre> 

Remember to set all the appropriate environment variables from the previous steps
<ol>
<li><code>JAVA_HOME</code></li>
<li><code>TOMCAT_HOME</code></li>
<li><code>ARCHAPPL_APPLIANCES</code></li>
<li><code>ARCHAPPL_MYIDENTITY</code></li>
<li><code>ARCHAPPL_SHORT_TERM_FOLDER</code> or equivalent</li>
<li><code>ARCHAPPL_MEDIUM_TERM_FOLDER</code> or equivalent</li>
<li><code>ARCHAPPL_LONG_TERM_FOLDER</code> or equivalent</li>
<li><code>JAVA_OPTS</code> - This is the environment variable typically used by Tomcat to pass arguments to the VM. You can pass in appropriate arguments like so
<pre><code>export JAVA_OPTS="-XX:MaxPermSize=128M -XX:+UseG1GC -Xmx4G -Xms4G -ea"</code></pre>
</li>
<li><code>LD_LIBRARY_PATH</code> - If you are using JCA, please make sure your LD_LIBRARY_PATH includes the paths to the JCA and EPICS base <code>.so</code>'s.
</li>
</ol>

A sample startup script using these elements is available <a href="samples/sampleStartup.sh">here</a>. Please modify to suit your installation.

</section>

<section>
<h3>Other containers</h3>
It is possible to deploy the 4 WAR files of the archiver appliance on other servlet containers or to use other industry standard provisioning software to provision an appliance. 
The details outlined here are guidelines on how to provision an appliance using Tomcat as a servlet container.
If you generate scripts for industry standard provisioning software and are willing to share them, please add them to the repositorty and contact the collaboration; we'll be happy to modify these documents to accomodate the same.
</section>



</div>

<script src="js/jquery/1.11.3/jquery.min.js"></script>
<script src="js/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="js/arch/docs.js"></script>

</body>
</html>
