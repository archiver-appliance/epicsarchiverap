<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link href="js/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
    <link type="text/css" href="docs.css" rel="Stylesheet" />
    <title>EPICS Archiver Appliance - Details</title>
  </head>
  <body>
    <!-- Header starts here -->
    <nav class="navbar navbar-inverse">
      <div class="container-fluid">
        <div class="navbar-header">
          <a class="navbar-brand" href="index.html"
            >The EPICS Archiver Appliance</a
          >
        </div>
        <div>
          <ul id="mainNavBar" class="nav navbar-nav">
            <li>
              <a href="https://github.com/slacmshankar/epicsarchiverap/wiki"
                >News</a
              >
            </li>
            <li><a href="details.html">Details</a></li>
            <li>
              <a
                href="https://github.com/slacmshankar/epicsarchiverap/releases/"
                >Download</a
              >
            </li>
            <li><a href="quickstart.html">Quickstart</a></li>
            <li><a href="installguide.html">Installation</a></li>
            <li><a href="userguide.html">User Guide</a></li>
            <li><a href="customization.html">Customize</a></li>
            <li><a href="developersguide.html">Developers</a></li>
            <li><a href="admin.html">Admin</a></li>
            <li><a href="api/index.html">Javadoc</a></li>
            <li>
              <a href="https://github.com/slacmshankar/epicsarchiverap/issues"
                >Issues</a
              >
            </li>
            <li><a href="faq.html">FAQ</a></li>
            <li><a href="license.html">License</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li>
              <img
                style="margin-top: -45px"
                src="images/Icon_Mathematical_Plot.png"
              />
            </li>
          </ul>
        </div>
      </div>
    </nav>
    <!-- Header ends here -->

    <div class="container">
      <div id="intro">
        This is an implementation of an archiver for
        <a href="http://www.aps.anl.gov/epics/index.php">EPICS</a> control
        systems that aims to archive millions of PVs.
      </div>

      <section>
        Here are the main features.
        <ul>
          <li>
            Ability to cluster appliances and to scale by adding appliances to
            the cluster.
            <ul>
              <li>
                Limited support for <a href="redundancy.html">redundancy</a>.
              </li>
            </ul>
          </li>
          <li>
            Multiple stages and an inbuilt process to move data between the
            stages.
            <ul>
              <li>
                This supports the ability to use faster storage (which is
                perhaps limited in size) to improve performance.
              </li>
              <li>
                Ability to reduce (decimate) the data as it moves into a store.
              </li>
            </ul>
          </li>
          <li>Focus on data retrieval performance.</li>
          <li>
            A management interface giving you the ability to manage and monitor
            the system using a browser. This includes
            <ol>
              <li>
                The ability to add PVs to a cluster of appliances using a
                browser (perhaps by users).
              </li>
              <li>Various metrics to help with capacity planning.</li>
              <li>
                Ability to define system-wide defaults for archiving parameters
                using policies.
              </li>
            </ol>
          </li>
          <li>
            Ability to script the business processes in the appliances using an
            external process in a language like
            <a href="http://www.python.org/">Python</a>.
          </li>
          <li>
            Ability to configure various archiving parameters on a per PV basis.
          </li>
          <li>
            Ability to customize the appliance to suit a different set of
            requirements. This includes
            <ul>
              <li>
                The ability to use alternate storage technologies that may
                better suit your needs or perform better in your environment.
              </li>
              <li>
                The ability to define your own data reduction algorithms and to
                optionally cache data generated by these algorithms on a per PV
                basis.
              </li>
              <li>
                Simple ways to add support for new MIME types in data retrieval
                responses.
              </li>
            </ul>
          </li>
          <li>Support for EPICS aliases.</li>
          <li>Support for EPICS 7/PVAccess/Structured data.</li>
          <li>
            Support for retrieval of data using
            <a href="csstudio.html">CS-Studio</a>, the
            <a href="archiveviewer.html">ArchiveViewer</a> and Matlab.
          </li>
          <li>
            Limited integration with existing Channel Archiver data sources.
          </li>
        </ul>
      </section>

      <section id="SystemRequirements">
        <h3>System requirements</h3>
        These are the prerequisites for the EPICS archiver appliance.
        <ul>
          <li>
            A recent version of Linux, definitely 64 bit Linux for production
            systems. If using RedHat, we should aim for RedHat 6.1.
          </li>
          <li>
            JDK 1.16+ - definitely the 64 bit version for production systems. We
            need the JDK, <b>not</b> the JRE.
          </li>
          <li>
            A recent version of Tomcat 9.x; preferably
            <code>apache-tomcat-9.0.20</code> or later.
          </li>
          <li>
            The management UI works best with a recent version of Firefox or
            Chrome.
          </li>
          <li>
            By default, the EPICS archiver appliance uses a bundled versions of
            the Java CA and PVA libraries from
            <a href="https://github.com/epics-base/epicsCoreJava">EPICS base</a
            >.
          </li>
        </ul>

        Optionally, we'd need
        <ul>
          <li>
            A recent version of MySQL <code>mysql-5.1</code> or later if
            persisting configuration to a database. We hope to add Postgres
            support soon.
          </li>
        </ul>

        In terms of hardware, for production systems, we'd need a reasonably
        powerful server box with lots of memory for each appliance. For example,
        we use 24 core machines with 128GB of memory and 15K SAS drives for
        medium term storage.
      </section>

      <section>
        <h3>Storage</h3>
        Out of the box, the following storage technologies/plugins are
        supported.
        <ul class="definition">
          <li>
            <span class="definitionterm"
              ><a
                href="api/edu/stanford/slac/archiverappliance/PlainPB/PlainPBStoragePlugin.html"
                >PlainPBStoragePlugin</a
              ></span
            >
            <span class="definition"
              >This plugin serializes samples using Google's
              <a href="https://developers.google.com/protocol-buffers"
                >ProtocolBuffers</a
              >
              and stores data in chunks. Each chunk has a well defined key and
              stores data for one PV for a well defined time duration (for
              example, a month). Using Java
              <a
                href="http://docs.oracle.com/javase/7/docs/api/java/nio/file/package-summary.html"
                >NIO2</a
              >, one can store each chunk in
            </span>
            <ol>
              <li>
                A file per chunk resulting in a file per PV per time partition.
              </li>
              <li>
                A zip file entry in a <code>.zip</code> file per chunk resulting
                in a <code>.zip</code> file per PV.
              </li>
              <li>
                This can be extended to use other storage technologies for which
                a NIO2 provider is available (for example,
                <a
                  href="https://github.com/Upplication/Amazon-S3-FileSystem-NIO2"
                  >Amazon S3</a
                >, a database BLOB per chunk or a key/value pair per chunk in
                any key/value store).
              </li>
            </ol>
            <div class="noteafterlist">
              By default, the PlainPBStoragePlugin maps PV names to keys using a
              simple algorithm that relies on the presence of a good PV naming
              convention. To use your own mapping scheme, see the
              <a href="customization.html#PVNamesToKeys">Key Mapping</a> section
              in the customization guide.
            </div>
          </li>
        </ul>
        To add support for other storage technologies - see the
        <a href="customization.html#key_mapping">customization guide</a> for
        details.
      </section>

      <section>
        <h3>Architecture</h3>
        Each appliance consists of 4 modules deployed in Tomcat containers as
        separate
        <a href="http://en.wikipedia.org/wiki/WAR_file_format_%28Sun%29">WAR</a>
        files. For production systems, it is recommended that each module be
        deployed in a separate Tomcat instance (thus yielding four Tomcat
        processes). A sample storage configuration is outlined below where we'd
        use
        <ol>
          <li>
            Ramdisk for the short term store - in this storage stage, we'd store
            data at a granularity of an hour.
          </li>
          <li>
            SSD/SAS drives for the medium term store - in this storage stage,
            we'd store data at a granularity of a day.
          </li>
          <li>
            A NAS/SAN for the long term store - in this storage stage, we'd
            store data at a granularity of a year.
          </li>
        </ol>
        <figure>
          <img class="img-responsive" src="images/applarch.png" />
          <figcaption>Architecture of a single appliance</figcaption>
        </figure>
        <div>
          A wide variety of such configurations is possible and supported. For
          example, if you have a powerful enough NAS/SAN, you could write
          straight to the long term store; bypassing all the stages in between.
        </div>

        <div>
          The long term store is shown outside the appliance as an example of a
          commonly deployed configuration. There is no necessity for the
          appliances to share any storage; so both of these configurations are
          possible.
          <figure>
            <img
              class="img-responsive"
              src="images/clusterinto1lts.png"
              alt="Multiple appliances into one long term store"
            />
            <figcaption>
              Multiple appliances sending data into one long term store
            </figcaption>
          </figure>
          <figure>
            <img
              class="img-responsive"
              src="images/clusterintodifflts.png"
              alt="Multiple appliances into different long term stores"
            />
            <figcaption>
              Multiple appliances sending data into different long term stores
            </figcaption>
          </figure>
        </div>
      </section>

      <section>
        <h3>Policies</h3>
        All of the various configurations can get quite tricky for end users to
        navigate. Rather than expose all of this variation to the end users and
        to provide a simple interface to end users, the archiver appliance uses
        <a
          href="api/org/epics/archiverappliance/mgmt/policy/package-summary.html"
          >policies</a
        >. Policies are Python scripts that make these decisions on behalf of
        the users. Policies are site-specific and identical across all
        appliances in the cluster. When a user requests a new PV to be archived,
        the archiver appliance samples the PV to determine event rate, storage
        rate and other parameters. In addition, various fields of the PV like
        .NAME, .ADEL, .MDEL, .RTYP etc are also obtained. These are passed to
        the policies python script which then has some simple code to configure
        the detailed archival parameters. The archiver appliance executes the
        <code>policies.py</code> python script using an embedded
        <a href="http://www.jython.org/">jython</a> interpreter. Policies allow
        system administrators to support a wide variety of configurations that
        are more appropriate to their infrastructure without exposing the
        details to their users.
      </section>

      <section>
        <h3>Clustering</h3>
        <p>
          While each appliance in a cluster is independent and self-contained,
          all members of a cluster are listed in a special configuration file
          (typically called
          <a href="installguide.html#appliances_xml">appliances.xml</a>) that is
          site-specific and identical across all appliances in the cluster. The
          <code>appliances.xml</code> is a simple XML file that contains the
          ports and URLs of the various webapps in that appliance. Each
          appliance has a dedicated TCP/IP endpoint called
          <code>cluster_inetport</code> for cluster operations like cluster
          membership etc.. One startup, the <code>mgmt</code> webapp uses the
          <code>cluster_inetport</code> of all the appliances in
          <code>appliances.xml</code> to discover other members of the cluster.
          This is done using TCP/IP only (no need for broadcast/multicast
          support).
        </p>

        <p>
          The business processes are all cluster-aware; the bulk of the
          inter-appliance communication that happens as part of normal operation
          is accomplished using JSON/HTTP on the other URLs defined in
          <code>appliances.xml</code>. All the JSON/HTTP calls from the mgmt
          webapp are also available to you for use in scripting, see the section
          on <a href="#scripting">scripting</a>.
        </p>

        <p>
          The archiving functionality is split across members of the cluster;
          that is, each PV that is being archived is being archived by one
          appliance in the cluster. However, both data retrieval and business
          requests can be dispatched to any random appliance in the cluster; the
          appliance has the functionality to route/proxy the request
          accordingly.
        </p>

        <figure>
          <img class="img-responsive" src="images/proxyrequest.png" />
          <figcaption>
            Appliance 1 proxies data retrieval request for PV being archived by
            appliance 2.
          </figcaption>
        </figure>
        In addition, users do not need to allocate PVs to appliances when
        requesting for new PVs be archived. The appliances maintain a small set
        of metrics during their operation and use this in addition to the
        measured event and storage rates to do an automated
        <a
          href="api/org/epics/archiverappliance/mgmt/archivepv/CapacityPlanningBPL.html"
          >Capacity Planning</a
        >/load balancing.
        <p></p>
      </section>

      <section id="scripting">
        <h3>Scripting</h3>
        The archiver appliance comes with a web UI that has support for various
        business processes like adding PV's to the archivers etc. The web UI
        communicates with the server principally using JSON/HTTP web service
        calls. The same web service calls are also available for use from
        external scripting tools like Python.
        <pre class="bash_output"><code>#!/usr/bin/env python

import requests

resp = requests.get("http://archappl.slac.stanford.edu/mgmt/bpl/getAllPVs?pv=VPIO:IN20:111:VRA*")
print("\n".join(resp.json()))</code></pre>

        Click <a href="api/mgmt_scriptables.html" target="_blank">here</a> for a
        list of business logic accesible thru scripting.
      </section>

      <section>
        <h3>EPICS 7</h3>
        The archiver appliance has built in support for EPICS 7 and archiving
        PV's over PVAccess. NTScalars and NTScalarArrays are stored as their
        channel access counterparts. For example, <code>PVDouble</code>'s will
        be stored as <code>DBR_SCALAR_DOUBLE</code>'s. This makes it possible to
        use standard archive viewers to view NTScalars and NTScalarArrays
        archived thru PVAccess. Other PVData types are stored as a bunch of bits
        using PVAccess serialization. While this is probably not the most
        efficient, it does allow for archiving of arbitrary structured data.
        There is support for retrieving of structured data using the
        <code>RAW</code> and <code>JSON</code> formats.
      </section>

      <section>
        <h3>Screenshots</h3>
        <figure>
          <img class="img-responsive" src="images/homepage.png" />
          <figcaption>A screenshot of the home page.</figcaption>
        </figure>

        <figure>
          <img class="img-responsive" src="images/reportspage.png" />
          <figcaption>We offer a wide variety of reports.</figcaption>
        </figure>

        <figure>
          <img class="img-responsive" src="images/metricspage.png" />
          <figcaption>Metrics maintained by the appliances.</figcaption>
        </figure>

        <figure>
          <img class="img-responsive" src="images/appliancespage.png" />
          <figcaption>
            The appliances page offers a quick view of some JVM parameters.
          </figcaption>
        </figure>
      </section>
    </div>

    <script src="js/jquery/1.11.3/jquery.min.js"></script>
    <script src="js/bootstrap-3.3.5/js/bootstrap.min.js"></script>
    <script src="js/arch/docs.js"></script>
  </body>
</html>
